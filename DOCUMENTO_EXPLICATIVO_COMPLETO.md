# CompareTablesProject - Documento Explicativo Completo

![CI](https://github.com/maite828/CompareTablesSparkScala/actions/workflows/ci.yml/badge.svg)

---

## üìã √çndice

1. [Visi√≥n General del Proyecto](#visi√≥n-general-del-proyecto)
2. [Arquitectura del Sistema](#arquitectura-del-sistema)
3. [Componentes Principales](#componentes-principales)
4. [Flujo de Comparaci√≥n](#flujo-de-comparaci√≥n)
5. [An√°lisis del C√≥digo Fuente](#an√°lisis-del-c√≥digo-fuente)
6. [Casos de Uso y Ejemplos](#casos-de-uso-y-ejemplos)
7. [Configuraci√≥n y Personalizaci√≥n](#configuraci√≥n-y-personalizaci√≥n)
8. [Rendimiento y Buenas Pr√°cticas](#rendimiento-y-buenas-pr√°cticas)
9. [Troubleshooting y Diagn√≥stico](#troubleshooting-y-diagn√≥stico)
10. [Mantenimiento y CI/CD](#mantenimiento-y-cicd)

---

## üéØ Visi√≥n General del Proyecto

**CompareTablesProject** es un motor de comparaci√≥n de tablas desarrollado en **Scala** y **Apache Spark 3.5**, dise√±ado para realizar an√°lisis detallados de diferencias entre datasets de referencia y nuevos.

### Objetivos Principales

- **Comparaci√≥n Fiel**: Mantener la integridad completa de los datos durante la comparaci√≥n
- **Detecci√≥n de Duplicados**: Identificar tanto duplicados exactos como variaciones
- **An√°lisis de Calidad**: Generar m√©tricas y KPIs para evaluar la calidad de los datos
- **Escalabilidad**: Manejar datasets de Big Data de manera eficiente
- **Flexibilidad**: Configuraci√≥n personalizable para diferentes casos de uso

### Tecnolog√≠as Utilizadas

- **Scala 2.13+**: Lenguaje de programaci√≥n funcional
- **Apache Spark 3.5**: Motor de procesamiento distribuido
- **Parquet**: Formato de almacenamiento columnar
- **Hive**: Metadatos y gesti√≥n de tablas
- **Excel**: Exportaci√≥n de res√∫menes (opcional)

---

## üèóÔ∏è Arquitectura del Sistema

### Componentes Principales

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    TableComparisonController                ‚îÇ
‚îÇ                     (Orquestador Principal)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ             ‚îÇ             ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇDiffGenerator ‚îÇ ‚îÇDuplicate‚îÇ ‚îÇSummary    ‚îÇ
‚îÇ              ‚îÇ ‚îÇDetector ‚îÇ ‚îÇGenerator  ‚îÇ
‚îÇ‚Ä¢ Comparaci√≥n ‚îÇ ‚îÇ‚Ä¢ Detecci√≥n‚îÇ ‚îÇ‚Ä¢ KPIs y   ‚îÇ
‚îÇ‚Ä¢ Diferencias ‚îÇ ‚îÇ‚Ä¢ Duplicados‚îÇ ‚îÇ‚Ä¢ M√©tricas ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ             ‚îÇ             ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   Output     ‚îÇ
              ‚îÇ   Tables     ‚îÇ
              ‚îÇ‚Ä¢ differences ‚îÇ
              ‚îÇ‚Ä¢ duplicates  ‚îÇ
              ‚îÇ‚Ä¢ summary     ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flujo de Datos

1. **Entrada**: Dos DataFrames (referencia y nuevo)
2. **Procesamiento**: Comparaci√≥n paralela y detecci√≥n de duplicados
3. **Salida**: Tres tablas de resultados + resumen ejecutivo
4. **Exportaci√≥n**: Opcional a Excel para an√°lisis posterior

---

## üîß Componentes Principales

### 1. DiffGenerator - Motor de Comparaci√≥n

**Responsabilidad**: Generar la tabla de diferencias entre dos datasets.

#### Funcionalidades Clave

```scala
object DiffGenerator {
  // Normalizaci√≥n de claves vac√≠as a NULL
  val norm: Column => Column = c => 
    when(trim(c.cast(StringType)) === "", lit(null)).otherwise(c)
  
  // Formateo fiel de valores respetando tipos de datos
  def formatValue(c: Column, dt: DataType): Column = dt match {
    case _: DecimalType =>
      val s = c.cast(StringType)
      when(s.isNull || trim(s) === "", lit("-")).otherwise(s)
    case _ =>
      val s = c.cast(StringType)
      when(s.isNull || trim(s) === "", lit("-")).otherwise(s)
  }
  
  // Canonicalizaci√≥n para comparaciones deterministas
  def canonicalize(c: Column, dt: DataType): Column = dt match {
    case _: NumericType   => c
    case _: StringType    => when(c.isNull, lit(null)).otherwise(c.cast(StringType))
    case mt: MapType      =>
      val sorted = array_sort(map_entries(c))
      to_json(map_from_entries(sorted))
    case _: ArrayType     => to_json(c)
    case _: StructType    => to_json(c)
    case BinaryType       => when(c.isNull, lit(null)).otherwise(encode(c.cast("binary"), "base64"))
    case _                => to_json(c)
  }
}
```

#### Proceso de Comparaci√≥n

1. **Normalizaci√≥n de Claves**: Valores vac√≠os se convierten a NULL
2. **Filtrado de Columnas Constantes**: Se excluyen autom√°ticamente
3. **Pre-ordenamiento**: Si hay columna de prioridad, se selecciona la fila de mayor prioridad
4. **Agregaci√≥n por Clave**: Se elige un valor representativo por columna
5. **Full Outer Join**: Comparaci√≥n completa entre ambos datasets
6. **Explosi√≥n de Diferencias**: Una fila por columna/clave con diferencias

### 2. DuplicateDetector - Detector de Duplicados

**Responsabilidad**: Identificar duplicados exactos y variaciones en ambos datasets.

#### L√≥gica de Detecci√≥n

```scala
object DuplicateDetector {
  def detectDuplicatesTable(
    spark: SparkSession,
    refDf: DataFrame,
    newDf: DataFrame,
    compositeKeyCols: Seq[String],
    config: CompareConfig
  ): DataFrame = {
    
    // 1. Unir y etiquetar origen
    val withSrc = refDf.withColumn("_src", lit("ref"))
      .unionByName(newDf.withColumn("_src", lit("new")))
    
    // 2. Aplicar prioridad si est√° definida
    val base = config.priorityCol match {
      case Some(prio) if withSrc.columns.contains(prio) =>
        val w = Window.partitionBy(("_src" +: compositeKeyCols).map(col): _*)
                   .orderBy(col(prio).desc_nulls_last)
        withSrc.withColumn("_rn", row_number().over(w))
               .filter(col("_rn") === 1)
               .drop("_rn")
      case _ => withSrc
    }
    
    // 3. Generar hash de fila para identificaci√≥n
    val hashCol = sha2(
      concat_ws("¬ß", base.columns.filter(_ != "_src").map { c =>
        coalesce(col(c).cast(StringType), lit("__NULL__"))
      }: _*),
      256
    )
    
    // 4. Agregaci√≥n y conteo
    val aggExprs = Seq(
      count(lit(1)).as("occurrences"),
      (count(lit(1)) - countDistinct("_row_hash")).as("exact_dup"),
      greatest(lit(0), countDistinct("_row_hash") - lit(1)).as("var_dup")
    )
  }
}
```

#### Tipos de Duplicados Detectados

- **Exact Duplicates**: Filas id√©nticas (mismo hash)
- **Duplicates with Variations**: Misma clave, valores diferentes
- **Occurrences**: Total de filas por grupo

### 3. SummaryGenerator - Generador de Res√∫menes

**Responsabilidad**: Crear m√©tricas ejecutivas y KPIs consolidados.

#### M√©tricas Generadas

```scala
case class SummaryRow(
  bloque: String,      // KPIS, MATCH, NO MATCH, GAP, DUPS
  metrica: String,     // Descripci√≥n de la m√©trica
  universo: String,    // REF, NEW, BOTH, ROWS
  numerador: String,   // Valor principal
  denominador: String, // Referencia para porcentajes
  pct: String,         // Porcentaje formateado
  ejemplos: String     // IDs de ejemplo para drill-down
)
```

#### Bloques de M√©tricas

1. **KPIS**: IDs √∫nicos, totales de filas, calidad global
2. **MATCH**: Coincidencias exactas en la intersecci√≥n
3. **NO MATCH**: Diferencias en la intersecci√≥n
4. **GAP**: Claves presentes solo en un lado
5. **DUPS**: Duplicados por origen

---

## üîÑ Flujo de Comparaci√≥n

### Paso 1: Preparaci√≥n de Datos

```scala
// Configuraci√≥n de la comparaci√≥n
val cfg = CompareConfig(
  spark               = spark,
  refTable            = "default.ref_customers",
  newTable            = "default.new_customers",
  partitionSpec       = Some("date=\"2025-07-01\"/geo=\"ES\""),
  compositeKeyCols    = Seq("id"),
  ignoreCols          = Seq("last_update"),
  initiativeName      = "Swift",
  tablePrefix         = "result_",
  checkDuplicates     = true,
  includeEqualsInDiff = true,
  autoCreateTables    = true,
  exportExcelPath     = Some("./output/summary.xlsx")
)
```

### Paso 2: Generaci√≥n de Diferencias

```scala
// DiffGenerator genera la tabla de diferencias
val diffDf = DiffGenerator.generateDifferencesTable(
  spark = spark,
  refDf = refDf,
  newDf = newDf,
  compositeKeyCols = cfg.compositeKeyCols,
  compareColsIn = compareCols,
  includeEquals = cfg.includeEqualsInDiff,
  config = cfg
)
```

### Paso 3: Detecci√≥n de Duplicados

```scala
// DuplicateDetector identifica duplicados
val dupDf = DuplicateDetector.detectDuplicatesTable(
  spark = spark,
  refDf = refDf,
  newDf = newDf,
  compositeKeyCols = cfg.compositeKeyCols,
  config = cfg
)
```

### Paso 4: Generaci√≥n del Resumen

```scala
// SummaryGenerator crea las m√©tricas ejecutivas
val summaryDf = SummaryGenerator.generateSummaryTable(
  spark = spark,
  refDf = refDf,
  newDf = newDf,
  diffDf = diffDf,
  dupDf = dupDf,
  compositeKeyCols = cfg.compositeKeyCols,
  refDfRaw = refDfRaw,
  newDfRaw = newDfRaw,
  config = cfg
)
```

---

## üìä An√°lisis del C√≥digo Fuente

### Estructura del Proyecto

```
src/main/scala/
‚îú‚îÄ‚îÄ Main.scala                    # Punto de entrada principal
‚îú‚îÄ‚îÄ TableComparisonController.scala # Controlador principal
‚îú‚îÄ‚îÄ CompareConfig.scala           # Configuraci√≥n del sistema
‚îú‚îÄ‚îÄ DiffGenerator.scala           # Motor de comparaci√≥n
‚îú‚îÄ‚îÄ DuplicateDetector.scala       # Detector de duplicados
‚îú‚îÄ‚îÄ SummaryGenerator.scala        # Generador de res√∫menes
‚îú‚îÄ‚îÄ SourceSpec.scala              # Especificaciones de fuentes
‚îî‚îÄ‚îÄ ...
```

### Patrones de Dise√±o Utilizados

1. **Object Pattern**: Cada componente es un objeto singleton
2. **Case Classes**: Para estructuras de datos inmutables
3. **Functional Programming**: Uso extensivo de funciones de orden superior
4. **Builder Pattern**: Configuraci√≥n flexible a trav√©s de CompareConfig

### Manejo de Tipos de Datos

#### Preservaci√≥n de Fidelidad

```scala
// Los tipos num√©ricos mantienen su precisi√≥n
case _: DecimalType =>
  val s = c.cast(StringType)
  when(s.isNull || trim(s) === "", lit("-")).otherwise(s)

// Los arrays preservan el orden
case _: ArrayType => to_json(c)

// Los mapas se ordenan para comparaci√≥n estable
case mt: MapType =>
  val sorted = array_sort(map_entries(c))
  to_json(map_from_entries(sorted))
```

#### Estrategias de Agregaci√≥n

```scala
// Configuraci√≥n personalizable de agregaciones
config.aggOverrides.get(c) match {
  case Some(MaxAgg)          => max(canon.cast(dt)).as(c)
  case Some(MinAgg)          => min(canon.cast(dt)).as(c)
  case Some(FirstNonNullAgg) => first(col(c), ignoreNulls = true).as(c)
  case None =>
    dt match {
      case _: NumericType | _: BooleanType | _: DateType | _: TimestampType =>
        max(canon.cast(dt)).as(c)
      case _ =>
        max(canon).as(c)
    }
}
```

---

## üéØ Casos de Uso y Ejemplos

### Caso 1: Comparaci√≥n Financiera

**Escenario**: Validar balances bancarios con precisi√≥n decimal cr√≠tica.

```scala
// Configuraci√≥n para datos financieros
val financialConfig = CompareConfig(
  // ... configuraci√≥n b√°sica ...
  aggOverrides = Map(
    "balance" -> MaxAgg,           // Siempre tomar el balance m√°s alto
    "last_update" -> MaxAgg,       // Fecha m√°s reciente
    "currency" -> FirstNonNullAgg   // Primer valor no-null
  )
)
```

**Resultado**: El sistema detecta diferencias m√≠nimas como `1000.000000000000000001` vs `1000.000000000000000000`.

### Caso 2: Validaci√≥n de C√≥digos de Producto

**Escenario**: Verificar c√≥digos donde `"ABC" ‚â† "abc"`.

```scala
// El sistema es case-sensitive por defecto
// Para hacer case-insensitive, modificar canonicalize:
case _: StringType => 
  when(c.isNull, lit(null))
    .otherwise(lower(c.cast(StringType)))
```

### Caso 3: Auditor√≠a de Transacciones

**Escenario**: Comparar timestamps con precisi√≥n de milisegundos.

```scala
// Los timestamps se comparan exactamente
case _: TimestampType => c
```

---

## ‚öôÔ∏è Configuraci√≥n y Personalizaci√≥n

### Par√°metros de CompareConfig

```scala
final case class CompareConfig(
  spark: SparkSession,                    // Sesi√≥n de Spark
  refSource: SourceSpec,                  // Fuente de referencia
  newSource: SourceSpec,                  // Fuente nueva
  partitionSpec: Option[String],          // Filtro de particiones
  compositeKeyCols: Seq[String],          // Columnas de clave compuesta
  ignoreCols: Seq[String],                // Columnas a ignorar
  initiativeName: String,                 // Nombre de la iniciativa
  tablePrefix: String,                    // Prefijo de tablas de resultado
  checkDuplicates: Boolean,               // Verificar duplicados
  includeEqualsInDiff: Boolean,           // Incluir matches en diferencias
  autoCreateTables: Boolean,              // Crear tablas autom√°ticamente
  nullKeyMatches: Boolean,                // NULLs en claves se consideran iguales
  includeDupInQuality: Boolean,           // Incluir duplicados en calidad
  priorityCol: Option[String],            // Columna de prioridad
  aggOverrides: Map[String, AggType],     // Overrides de agregaci√≥n
  exportExcelPath: Option[String]         // Ruta de exportaci√≥n a Excel
)
```

### Estrategias de Agregaci√≥n Personalizables

```scala
sealed trait AggType
object AggType {
  case object MaxAgg          extends AggType
  case object MinAgg          extends AggType
  case object FirstNonNullAgg extends AggType
}

// Ejemplo de uso
val config = CompareConfig(
  // ... configuraci√≥n b√°sica ...
  aggOverrides = Map(
    "amount" -> MaxAgg,                    // Siempre m√°ximo
    "status" -> FirstNonNullAgg,           // Primer valor no-null
    "country" -> MinAgg,                   // Siempre m√≠nimo
    "last_update" -> MaxAgg                // Fecha m√°s reciente
  )
)
```

---

## üöÄ Rendimiento y Buenas Pr√°cticas

### Optimizaciones Implementadas

1. **Filtrado de Columnas Constantes**: Se excluyen autom√°ticamente
2. **Pre-ordenamiento por Prioridad**: Reduce el procesamiento posterior
3. **Agregaci√≥n Eficiente**: Una sola pasada por los datos
4. **Hash SHA-256**: Para identificaci√≥n r√°pida de duplicados

### Recomendaciones de Rendimiento

```scala
// 1. Filtrar por particiones siempre que sea posible
partitionSpec = Some("date=\"2025-07-01\"/geo=\"ES\"")

// 2. Limitar columnas a comparar
ignoreCols = Seq("last_update", "created_by", "temp_field")

// 3. Usar columnas de prioridad para estabilizar resultados
priorityCol = Some("timestamp")

// 4. Configurar paralelismo de Spark
spark.conf.set("spark.sql.shuffle.partitions", "200")
```

### Escalabilidad

- **Objetivo**: Tablas de 10-50M filas con 10-50 columnas
- **Tiempo t√≠pico**: Minutos en cluster mediano
- **Memoria**: Optimizado para evitar OOM en datasets grandes

---

## üîç Troubleshooting y Diagn√≥stico

### Cheat-Sheet de Diagn√≥stico R√°pido

| S√≠ntoma | Primera Revisi√≥n | Filtro/Ordenaci√≥n | Acci√≥n T√≠pica |
|---------|------------------|-------------------|---------------|
| `% NO MATCH` alto en BOTH | `result_differences` | `results = 'NO_MATCH'` ‚Üí ordena por `id, column` | Revisar normalizaci√≥n y reglas de agregaci√≥n |
| Muchos `ONLY_IN_REF` o `ONLY_IN_NEW` | `result_differences` | `results LIKE 'ONLY_IN_%'` | Verificar filtros de partici√≥n y fechas |
| `duplicates_w_variations` alto | `result_duplicates` | `var_dup > 0` | Revisar procesos upstream y reglas de consolidaci√≥n |
| `exact_duplicates` alto | `result_duplicates` | `exact_dup > 0` | Identificar copias exactas para deduplicaci√≥n |

### Flujo de Investigaci√≥n (3 Pasos)

```
[1] result_summary (KPIs) 
   ‚îú‚îÄ ¬øGAP alto? ‚Üí Revisar particiones/fechas/filtros
   ‚îú‚îÄ ¬øDUPS alto? ‚Üí Ir a result_duplicates
   ‚îî‚îÄ ¬øNO MATCH alto en BOTH? ‚Üí Ir a result_differences

[2] result_differences (detalle de cambios)
   ‚îú‚îÄ Filtrar results != 'MATCH' (id, column)
   ‚îú‚îÄ ¬øStrings? revisar espacios/may√∫sculas
   ‚îú‚îÄ ¬øNums/fechas? revisar escala y agregaci√≥n
   ‚îî‚îÄ Si clave estable pero hay ruido ‚Üí ir a result_duplicates

[3] result_duplicates (por qu√© hay ruido)
   ‚îú‚îÄ exact_dup > 0 ‚Üí copias exactas ‚Üí deduplicar
   ‚îî‚îÄ var_dup > 0 ‚Üí reescrituras ‚Üí reglas de consolidaci√≥n
```

### Se√±ales de Alerta

- **`only_in_*` masivo**: Filtros de partici√≥n incorrectos
- **`var_dup` alto**: Procesos upstream que reescriben claves
- **`% NO_MATCH` alto en BOTH**: Normalizaci√≥n mal definida

---

## üõ†Ô∏è Mantenimiento y CI/CD

### Tests y Validaci√≥n

```bash
# Ejecutar tests unitarios
sbt test

# Ejecutar comparaci√≥n completa
./run_compare.sh

# Validar configuraci√≥n
./scripts/bootstrap.sh
```

### GitHub Actions

```yaml
# .github/workflows/ci.yml
name: CI
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up JDK 11
        uses: actions/setup-java@v3
        with:
          java-version: '11'
          distribution: 'temurin'
      - name: Run tests
        run: sbt test
```

### Snapshots y Validaci√≥n

- **Tests unitarios**: 11 pruebas (<15s)
- **Tests de integraci√≥n**: Validaci√≥n end-to-end
- **Snapshots dorados**: Actualizaci√≥n tras cambios de l√≥gica

---

## üìà M√©tricas y KPIs

### Interpretaci√≥n de Resultados

#### Tabla `result_differences`

| Resultado | Significado |
|-----------|-------------|
| `MATCH` | Valor id√©ntico en REF y NEW |
| `NO_MATCH` | Clave existe en ambos, pero valores difieren |
| `ONLY_IN_REF` | Clave solo existe en referencia |
| `ONLY_IN_NEW` | Clave solo existe en nuevo |

#### Tabla `result_duplicates`

| M√©trica | Interpretaci√≥n |
|---------|----------------|
| `exact_duplicates` | Filas id√©nticas (hash repetido) |
| `duplicates_w_variations` | Filas con al menos una diferencia |
| `occurrences` | Total de filas por grupo |

#### Tabla `result_summary`

| Bloque | M√©tricas |
|--------|----------|
| **KPIS** | IDs √∫nicos, totales, calidad global |
| **MATCH** | Coincidencias exactas en intersecci√≥n |
| **NO MATCH** | Diferencias en intersecci√≥n |
| **GAP** | Claves presentes solo en un lado |
| **DUPS** | Duplicados por origen |

---

## üîÆ Futuras Mejoras

### Funcionalidades Planificadas

1. **Comparaci√≥n Incremental**: Solo procesar cambios desde √∫ltima ejecuci√≥n
2. **Alertas Autom√°ticas**: Notificaciones cuando calidad cae por debajo de umbrales
3. **Dashboard Web**: Interfaz gr√°fica para an√°lisis de resultados
4. **Integraci√≥n con Data Quality Tools**: Conectores con Great Expectations, Deequ
5. **Soporte para Streaming**: Comparaci√≥n en tiempo real

### Optimizaciones T√©cnicas

1. **Broadcast Joins**: Para dimensiones peque√±as
2. **Skew Join Hints**: Manejo de claves muy calientes
3. **Compresi√≥n Avanzada**: Optimizaci√≥n de almacenamiento Parquet
4. **Cache Inteligente**: Reutilizaci√≥n de DataFrames intermedios

---

## üìö Recursos Adicionales

### Documentaci√≥n T√©cnica

- [README Ejecutivo](README_EJECUTIVO.md)
- [Flujo Funcional](FLUJO_FUNCIONAL.md)
- [README Original](README_ORIGINAL.md)

### Ejemplos de Uso

- [Scripts de Ejecuci√≥n](scripts/)
- [Tests Unitarios](src/test/)
- [Configuraciones de Ejemplo](src/main/scala/)

### Comunidad y Soporte

- **GitHub**: [maite828/CompareTablesSparkScala](https://github.com/maite828/CompareTablesSparkScala)
- **Issues**: Reportar bugs y solicitar features
- **Discussions**: Preguntas y respuestas de la comunidad

---

## üèÅ Conclusi√≥n

**CompareTablesProject** representa una soluci√≥n robusta y escalable para la comparaci√≥n de datasets en entornos Big Data. Su arquitectura modular, configuraci√≥n flexible y preservaci√≥n de fidelidad de datos lo convierten en una herramienta esencial para:

- **Data Quality**: Validaci√≥n y control de calidad de datos
- **Data Reconciliation**: Conciliaci√≥n entre sistemas
- **Change Detection**: Detecci√≥n de cambios en datasets
- **Audit Trail**: Trazabilidad de modificaciones

El proyecto demuestra las mejores pr√°cticas de desarrollo en Scala y Spark, con un enfoque en mantenibilidad, rendimiento y usabilidad.

---

**¬© 2025 ¬∑ CompareTables Spark 3.5.0 ¬∑ MIT License**

*Desarrollado con ‚ù§Ô∏è para la comunidad de Big Data*
